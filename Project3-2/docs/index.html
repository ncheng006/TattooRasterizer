<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Michelle Chen, Jojo Chen</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">TODO</a></h2>

<br><br>

<h2 align="middle">Overview</h2>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
We used the ray tracing technique to render our image. First, we generated camera rays by transforming the given normalized image coordinates into the camera space, generating a ray in the camera space, then converting that ray to one in the world space. Our image coordinates are normalized and the sensor is centered at (0,0,1) with a bottom left corner at (-tan(hFov/2), -tan(vFov/2), -1) and the top right corner at (-tan(hFov/2), -tan(vFov/2), -1). Therefore, in order to translate our normalized coordinates to the camera space coordinates, we calculated the new x position with 2 * tan(hFov/2) * (x-0.5) and y position with 2 * tan(vFov/2) * (y-0.5), making sure to convert hFov and cFov to radians. We then used the c2w rotation matrix and generated a Ray object in the world space.
</p>
<p>We implemented pixel samples by generating a ray for ns_aa number of samples per pixel, then averaging the value returned by est_radiance_global_illumination for each ray. We then updated the sampleBuffer with this averaged value, effectively calculating the integral of radiance over the pixel.
</p>
<p>Next we implemented ray-surface intersections. For both triangles and spheres, we wrote functions that determined if a ray hit the object and saved the intersection’s information. To implement the ray-triangle intersection methods, we used the Moller Trumbore algorithm to determine whether and where the ray intersects with the triangle, which will be explained in the next section. We wrote a helper method that ran the algorithm and returned a Vector3D with the results. In has_intersection(), we checked to see if the resulting t is between the min_t and max_t, as well as if the resulting barycentric coordinates b0, b1, and b2 are between 0 and 1. If all these variables were valid, we would update the max_t to this new t and then return true, otherwise we would know that there is not a valid intersection. In intersect(), we did the same validity checks and also updated the Intersection object if a valid intersection was found. Notably, we calculated the surface normal at the intersection by using the barycentric coordinates generated by the Moller Trumbore algorithm and calculating the weighted average of the three vertex normals of the triangle. </p>
<br>
<p>We also implemented ray-sphere intersection functions in a similar way. In the test() helper function, we used the following formulas to calculate the a, b, and c values which we plugged into the quadratic formula to find possible intersections. This formula is found by setting the ray equation equal to the sphere equation. </p>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/sphere-equations.png" align="middle" width="400px"/>
      </td>
      <td>
        <img src="images/quad-equation.png" align="middle" width="400px"/>
      </td>
    </tr>
  </table>
</div>
<p>We then make sure that t1 and t2 store any valid intersections, meaning that they are between min_t and max_t in ascending order. Has_intersection() and intersection() both return t1 as the intersection if a valid intersection exists. Intersection() stores the surface normal, calculated as the position of r at time t1 minus the sphere center, normalized. </p>
<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
We used the Moller Trumbore algorithm to calculate the ray-triangle intersection. The algorithm works by setting the ray equation (O + tD) to (1-b1-b2)P0 + b1P1 + b2p2 where b1, b2, and b0 (1-b1-b2) are barycentric coordinates. The algorithm then solves for t, b1 and b2 using the equations shown in the image, and returns the results in a vector. Here, the intersection is only valid if t is between the min_t and max_t of the ray. B0, b1, and b2 must all be between 0 and 1 (inclusive) or the point lies outside of the triangle.
</p>
<div align="middle">
  <img src="images/mt-equations.png" align="middle" width="400px"/>
  </div>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="images/banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBbunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/CBgems.png" align="middle" width="400px"/>
        <figcaption>CBgems.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  Our BVH construction algorithm builds off of the provided starter code. For leaf nodes, where the number of input primitives was less than or equal to the input max_leaf_size, we simply set their start and end pointers to the input start and end iterators since they will contain all the input primitives. We also set the left and right pointers to null values since leaf nodes will not have children.
  <br>
  <br> 
  For inner nodes, we decided to base our split point on the average of the centroids along the longest axis. We track the sum of the centroids as we iterated through the primitives in the original for loop. According to the longest axis, we use std::partition to split our primitives across the average centroid value and save the iterator at the midpoint returned by the function. Finally, we recursively call construct_bvh to construct the left and right BVHNodes which go from start to mid_point and mid_point to end respectively.</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/beast.png" align="middle" width="300px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="images/maxplanck.png" align="middle" width="300px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBlucy.png" align="middle" width="300px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
      <td>
        <img src="images/blob.png" align="middle" width="300px"/>
        <figcaption>blob.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    We rendered the following moderately complex meshes with and without BVH: cow.dae, beetle.dae, CBcoil.dae, bunny.dae
    <br>
    <br>
    Without BVH they took: cow.dae (16.54s), beetle.dae (20.97s), CBcoil.dae (22.27s), bunny.dae (153.17s)
    <br>
    <br>
    With BVH they took: cow.dae (0.033s), beetle.dae (0.028s), CBcoil.dae (0.039s), bunny.dae (0.034s)
    <br>
    <br>
    Without BVH, the rendering times scaled noticeably with mesh size. The simplest of the four, cow.dae (5356 primitives), rendered in 16.5 seconds. CBcoil.dae, which was about 1.3x its size rendered in 22.3 seconds, while bunny.dae, about 5.7x its size, rendered in 153.2 seconds, over 9x cow.dae's rendering time. With BVH acceleration, all four meshes had pretty consistent render times (about 0.03s on average) regardless of their size. It’s pretty likely that the differences in rendering time are caused by the number of primitive integration tests run. We noticed that the number of intersection tests per ray increased linearly with the primitive size when rendering without BVH. With BVH implemented, however, we are able to effectively skip intersection tests on rays that miss the bounding box and only actually check primitives in leaf nodes with bounding boxes intersected by the rays. Since the leaf nodes were designed to have minimal overlap, this greatly reduces the number of tests needed to check for an actual intersection. So it makes sense that our BVH pathtracer would be able to handle size increases much more efficiently. 
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  <h4>Uniform Hemisphere Sampling</h4>
  With this method, we estimate the direct lighting on a point by sampling uniformly in a hemisphere. For each one of our num_samples samples, we sample a point in our hemisphere using the HemisphereSampler and translate it into our world space. We then cast a ray from the hit point to our sample in the hemisphere and check for any intersections in our BVH using the bvh->intersect method implemented in Task 2. If there is an intersection, we find the radiance of the emitted light according to the BSDF of the intersection point and a cosine term (<code>e * isect.bsdf->f(w_out, sample) * 2 * PI * cos_theta(sample)</code>) and add it to our total L_out. Finally, we divide L_out by our number of samples and return.
<br>
<br>
  <h4>Lighting Sampling</h4>
  With lighting sampling, we loop through all of the light sources and determine the num_samples we want to take from each source. If the light is a delta light, we only take one sample from it. For each of the num_samples samples, we calculate the radiance rad, direction vector in world coordinates wi , the distance from the hit point to the light source, and the probability density function pdf. We then convert wi into our object space coordinates and check if it points to our hit point. If it does, we create a new ray and check for intersections between it and any primitives in our BVH. If there are no objects between the light source and our hit point, we calculate the incoming radiance <code>(rad * isect.bsdf->f(w_out, w_in) * cos_theta(w_in))/pdf</code> and add it to our total L_out. Like before, we divide L_out by our number of samples and return.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/CBbunny-hemisphere.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae with uniform hemisphere sampling</figcaption>
      </td>
      <td>
        <img src="images/CBbunny-direct.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae with lighting sampling</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/sphere-hem.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae with uniform hemisphere sampling</figcaption>
      </td>
      <td>
        <img src="images/sphere-imp.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae with lighting sampling</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny-1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny-4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny-16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny-64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  Using fewer light rays results in more noise and harder shadows. In the image with 1 light ray, you can see that the shadows on the bunny and underneath the bunny appear grainy, with specks of black pixels scattered between the boundaries of lighter areas and darker areas. With 16 and especially with 64 rays, the shadows are much more realistic and there are pixels with transition shades to form the edges of the shadows. 
  <br>
  <div align="middle">
    <table style="width:100%">
      <tr align="center">
        <td>
          <img src="images/bunny-foot-1.png" align="middle" width="200px"/>
        </td>
        <td>
          <img src="images/bunny-foot-64.png" align="middle" width="200px"/>
        </td>
      </tr>
    </table>
  </div>
  <br>
  This is because increasing the number of rays also increases the accuracy of the lighting information sampled at each location, resulting in softer shadows and a more realistic result.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    Both uniform hemisphere sampling and lighting sampling are able to capture similar levels of detail in the meshes. The most noticeable difference visually is that uniform hemisphere sampling produces a more grainy result (similar to what we saw with lighting sampling using fewer light rays) while lighting sampling results in a smoother result with softer shadows. Since lighting sampling accounts for the distribution of area lights in the scene, it calculates lighting more accurately, which is probably also why the results are generally brighter looking. The light source itself for uniform sampling also appears to have blurred edges, probably also a result of how the sampling is performed. Performance-wise, however, uniform hemisphere sampling renders faster than lighting sampling because it requires less computation.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
In order to implement the global illumination method, we first implemented the sample_f() function in the BDSF file, where much like f(), we return reflectance/PI as the f(wi-> wo). Additionally, this function samples the incoming ray with the given pdf as the sample probability and sets wi to the resulting sample returned by sampler.get_sample(pdf).
</p>
<p>
We made sure to update est_radiance_global_illumination to return both the light (zero bounce radiance) and the indirect lighting (at least one bounce radiance), adding to the direct lighting functions we implemented in part 3.
</p>
<p>
In the at_least_one_bounce_radiance() function, we implemented the bulk of our indirect lighting logic. This function returns the result of one_bounce_radiance, implemented in part 3 combined with any other bounces beyond the first bounce by recursively calling itself. We used the russian roulette method with a termination probability of 0.35 to stop infinite recursion since it is computationally infeasible to calculate all bounces of the lights. For each sample taken with sample_f(), we trace its subsequent bounces only if 1) Russian roulette returns or 2) max_ray_depth > 1. Additionally, we only trace bounces if this ray intersects the scene, which we determine with bvh->intersect(). If these conditions are met, we recursively add the results of the following equation to our result: at_least_one_bounce_radiance(resultRay, Intersection) +  p.brdf(wi, -w) * costheta / pdf /cdf.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example.dae</figcaption>
      </td>
      <td>
        <img src="images/spheres-direct-indirect.png" align="middle" width="400px"/>
        <figcaption>CBspheres-lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres-direct.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres-lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres-indirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (Bspheres-lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Since the indirect illumination only shows the light as a result of bounces without the direct lighting, it is very dark. On the other hand, the direct illumination image is not as bright as the original image because it excludes the lighting that results from subsequent light bounces. 
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny-max-0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny-max-1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny-max-2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny-max-3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny-max-100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres-lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
As the sample rate increased, the amount of noise in the image decreased significantly. At 1 sample per pixel, there are many white dots that interfere heavily with the visual clarity of the image. At 16 samples per pixel, the visual clarity is a lot better, but there is still a decent amount of noise. At 1024 samples per pixel, there is no discernible noise and everything has a smooth look.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
In order to decrease noise in our images, we can increase the number of samples we take for every pixel. However, this is not very efficient since some pixels can converge with a lower number of samples. Adaptive sampling tries to be more efficient in eliminating noise by sampling more in more complex areas, while sampling less in less complex areas.
</p>
<p>
The adaptive sampling algorithm we implemented works as follows. For every samplesPerBatch samples we take, we check to see if a pixel has converged up to ns_aa samples. We keep track of s1 and s1, which store the summation of every sample’s illuminance and the summation of the square of every sample’s illuminance. Every time we check for convergence, we calculate the mean and variance as follows.
</p>
<img src="images/mean_var_formula.png" align="middle" width="400px"/>
<p>
We then define a variable, I, as follows.
</p>
<img src="images/i_formula.png" align="middle" width="200px"/>
<p>
We check if I <= maxTolerance * mean is true, if it is, then the pixel has converged and no further sampling is required. We then stop sampling this pixel, and find and store its average.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres-lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres-lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
